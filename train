#!/usr/bin/env python
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
import argparse

parser = argparse.ArgumentParser(description='doc2vec model trainer')
parser.add_argument(
    '-e', '--epochs',
    dest='epochs',
    type=int,
    metavar='EPOCHS',
    help='number of epochs to train the model (default: 100)',
    default=100
)
parser.add_argument(
    '-d', '--output_dimensions',
    dest='vector_size',
    type=int,
    metavar='DIMENSIONS',
    help='number of output dimensions to embed (default: 300)',
    default=300
)
parser.add_argument(
    '-a', '--alpha',
    dest='alpha',
    type=float,
    metavar='ALPHA',
    help='learning rate (default: 0.025)',
    default=300
)
parser.add_argument(
    '--decay',
    dest='decay',
    type=float,
    metavar='DECAY',
    help='decay to the learning rate (default: 0.0002)',
    default=0.0002
)
parser.add_argument(
    'input_path',
    metavar='INPUT',
    help='source file, list of newline-delimited sentences or phrases, e.g. sentences.csv'
)
parser.add_argument(
    'output_model',
    metavar='OUTPUT_MODEL',
    help='model output path, e.g. model.d2v'
)
parser.add_argument(
    'output_vectors',
    metavar='OUTPUT_VECTORS',
    help='vector output path in word2vec format, e.g. sentences.vec'
)

args = parser.parse_args()
print(args)

print('loading sentences/phrases')
with open(args.input_path) as f:
    sentences = f.read().splitlines()

print('tagging sentences/phrases')
tagged = [
    TaggedDocument(
        words=word_tokenize(_d.lower()),
        tags=[str(i)]
    ) for i, _d in enumerate(sentences)
]

print('training d2v model')
model = Doc2Vec(
    vector_size=args.vector_size,
    alpha=args.alpha,
    min_alpha=0.00025,
    min_count=1,
    dm=1
)
model.build_vocab(tagged)

for epoch in range(args.epochs):
    print('> {}/{}'.format(epoch + 1, args.epochs), end='\r')
    model.train(
        tagged,
        total_examples=model.corpus_count,
        epochs=model.epochs
    )
    # decrease the learning rate, improves single label performance
    # https://rare-technologies.com/doc2vec-tutorial/
    model.alpha -= args.decay
    model.min_alpha = model.alpha

print('saving d2v model')
model.save(args.output_model)

print('saving sentence/phrase vectors')
model.docvecs.save_word2vec_format(args.output_vectors)
